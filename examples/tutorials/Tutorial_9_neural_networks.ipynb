{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a079f613",
   "metadata": {},
   "source": [
    "# Tutorial 9: Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bb2c0aa",
   "metadata": {
    "papermill": {
     "duration": 0.032379,
     "end_time": "2021-06-22T20:10:29.835505",
     "exception": false,
     "start_time": "2021-06-22T20:10:29.803126",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"../../docs/imgs/lightautoml_logo_color.png\" alt=\"LightAutoML logo\" style=\"width:100%;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ab03e9a",
   "metadata": {},
   "source": [
    "Official LightAutoML github repository is [here](https://github.com/AILab-MLTools/LightAutoML)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72cea480",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial you will learn how to:\n",
    "* train neural networks (nn) with LightAutoML on tabualr data\n",
    "* customize model architecture and pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb2f44",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc8496d8",
   "metadata": {},
   "source": [
    "### 0.0 install LightAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58201c72",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-06-22T20:10:29.980264Z",
     "iopub.status.busy": "2021-06-22T20:10:29.979511Z",
     "iopub.status.idle": "2021-06-22T20:10:52.955439Z",
     "shell.execute_reply": "2021-06-22T20:10:52.953955Z",
     "shell.execute_reply.started": "2021-06-22T19:06:24.534180Z"
    },
    "papermill": {
     "duration": 23.023261,
     "end_time": "2021-06-22T20:10:52.955691",
     "exception": false,
     "start_time": "2021-06-22T20:10:29.932430",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U lightautoml[all]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e1606cb",
   "metadata": {
    "papermill": {
     "duration": 0.066681,
     "end_time": "2021-06-22T20:10:53.090975",
     "exception": false,
     "start_time": "2021-06-22T20:10:53.024294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 0.1 Import libraries\n",
    "\n",
    "Here we will import the libraries we use in this kernel:\n",
    "- Standard python libraries for timing, working with OS etc.\n",
    "- Essential python DS libraries like numpy, pandas, scikit-learn and torch (the last we will use in the next cell)\n",
    "- LightAutoML modules: presets for AutoML, task and report generation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bea2ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T20:10:53.233356Z",
     "iopub.status.busy": "2021-06-22T20:10:53.232675Z",
     "iopub.status.idle": "2021-06-22T20:11:01.486841Z",
     "shell.execute_reply": "2021-06-22T20:11:01.487566Z",
     "shell.execute_reply.started": "2021-06-22T19:06:43.597648Z"
    },
    "papermill": {
     "duration": 8.32949,
     "end_time": "2021-06-22T20:11:01.487788",
     "exception": false,
     "start_time": "2021-06-22T20:10:53.158298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvladimirvasilyev/anaconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nlp' extra dependecy package 'gensim' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'gensim' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'nltk' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n",
      "'nlp' extra dependecy package 'transformers' isn't installed. Look at README.md in repo 'LightAutoML' for installation instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvladimirvasilyev/LightAutoML/lightautoml/ml_algo/dl_model.py:41: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n",
      "/home/dvladimirvasilyev/LightAutoML/lightautoml/text/nn_model.py:22: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n",
      "/home/dvladimirvasilyev/LightAutoML/lightautoml/text/dl_transformers.py:25: UserWarning: 'transformers' - package isn't installed\n",
      "  warnings.warn(\"'transformers' - package isn't installed\")\n"
     ]
    }
   ],
   "source": [
    "# Standard python libraries\n",
    "import os\n",
    "\n",
    "# Essential DS libraries\n",
    "import optuna\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from copy import deepcopy as copy\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# LightAutoML presets, task and report generation\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML\n",
    "from lightautoml.tasks import Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "486dff3d",
   "metadata": {
    "papermill": {
     "duration": 0.064234,
     "end_time": "2021-06-22T20:11:01.619010",
     "exception": false,
     "start_time": "2021-06-22T20:11:01.554776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 0.2 Constants\n",
    "\n",
    "Here we setup the constants to use in the kernel:\n",
    "- `N_THREADS` - number of vCPUs for LightAutoML model creation\n",
    "- `N_FOLDS` - number of folds in LightAutoML inner CV\n",
    "- `RANDOM_STATE` - random seed for better reproducibility\n",
    "- `TEST_SIZE` - houldout data part size \n",
    "- `TIMEOUT` - limit in seconds for model to train\n",
    "- `TARGET_NAME` - target column name in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64dfd5d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T20:11:01.758476Z",
     "iopub.status.busy": "2021-06-22T20:11:01.757403Z",
     "iopub.status.idle": "2021-06-22T20:11:01.760870Z",
     "shell.execute_reply": "2021-06-22T20:11:01.760168Z",
     "shell.execute_reply.started": "2021-06-22T19:06:51.523697Z"
    },
    "papermill": {
     "duration": 0.077787,
     "end_time": "2021-06-22T20:11:01.761030",
     "exception": false,
     "start_time": "2021-06-22T20:11:01.683243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_THREADS = 4\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "TIMEOUT = 300\n",
    "TARGET_NAME = 'TARGET'\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62e42740",
   "metadata": {},
   "source": [
    "### 0.3 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c3218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '../data/'\n",
    "DATASET_NAME = 'sampled_app_train.csv'\n",
    "DATASET_FULLNAME = os.path.join(DATASET_DIR, DATASET_NAME)\n",
    "DATASET_URL = 'https://raw.githubusercontent.com/AILab-MLTools/LightAutoML/master/examples/data/sampled_app_train.csv'\n",
    "\n",
    "if not os.path.exists(DATASET_FULLNAME):\n",
    "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "    dataset = requests.get(DATASET_URL).text\n",
    "    with open(DATASET_FULLNAME, 'w') as output:\n",
    "        output.write(dataset)\n",
    "\n",
    "data = pd.read_csv(DATASET_FULLNAME)\n",
    "data.head()\n",
    "\n",
    "tr_data, te_data = train_test_split(\n",
    "    data, \n",
    "    test_size=TEST_SIZE, \n",
    "    stratify=data[TARGET_NAME], \n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "541535c0",
   "metadata": {},
   "source": [
    "## 1. Available built-in models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78b11e",
   "metadata": {},
   "source": [
    "To use different model pass it to the list in `\"use_algo\"`. We support custom models inherited from `torch.nn.Module` class. For every model their parameters is listed below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb1da404",
   "metadata": {},
   "source": [
    "### 1.1 MLP (`\"mlp\"`)\n",
    "- `hidden_size` - define hidden layer dimensions\n",
    "\n",
    "### 1.2 Dense Light (`\"denselight\"`)\n",
    "<img src=\"../../docs/imgs/denselight.png\" style=\"width:25%;\"/>\n",
    "\n",
    "- `hidden_size` - define hidden layer dimensions\n",
    "\n",
    "### 1.3 Dense (`\"dense\"`)\n",
    "<img src=\"../../docs/imgs/densenet.png\" style=\"width:60%;\"/>\n",
    "\n",
    "- `block_config` - set number of blocks and layers within each block\n",
    "- `compression` - portion of neuron to drop after `DenseBlock`\n",
    "- `growth_size` - output dim of every `DenseLayer`\n",
    "- `bn_factor` - size of intermediate fc is increased times this factor in layer\n",
    "\n",
    "### 1.4 Resnet (`\"resnet\"`)\n",
    "<img src=\"../../docs/imgs/resnet.png\" style=\"width:50%;\"/>\n",
    "\n",
    "- `hid_factor` - size of intermediate fc is increased times this factor in layer\n",
    "\n",
    "### 1.5 SNN (`\"snn\"`)\n",
    "- `hidden_size` - define hidden layer dimensions\n",
    "\n",
    "### 1.5 NODE (`\"node\"`)\n",
    "<img src=\"../../docs/imgs/node.png\" style=\"width:80%;\"/>\n",
    "\n",
    "### 1.5 AutoInt (`\"autoint\"`)\n",
    "<img src=\"../../docs/imgs/autoint.png\" style=\"width:80%;\"/>\n",
    "\n",
    "### 1.5 FTTransformer (`\"fttransformer\"`)\n",
    "<img src=\"../../docs/imgs/fttransformer.png\" style=\"width:80%;\"/>\n",
    "\n",
    "- `pooling` - Pooling used for the last step.\n",
    "- `n_out` - Output dimension, 1 for binary prediction.\n",
    "- `embedding_size` - Embeddings size.\n",
    "- `depth` - Number of Attention Blocks inside Transformer.\n",
    "- `heads` - Number of heads in Attention.\n",
    "- `attn_dropout` - Post-Attention dropout.\n",
    "- `ff_dropout` - Feed-Forward Dropout.\n",
    "- `dim_head` - Attention head dimension\n",
    "- `return_attn` - Return attention scores or not.\n",
    "- `num_enc_layers` - Number of Transformer layers.\n",
    "- `device` - Device to compute on.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7266e9d9",
   "metadata": {},
   "source": [
    "## 2. Example of usage\n",
    "### 2.1 Task definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc3bd7a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T20:11:23.005952Z",
     "iopub.status.busy": "2021-06-22T20:11:23.002234Z",
     "iopub.status.idle": "2021-06-22T20:11:23.009732Z",
     "shell.execute_reply": "2021-06-22T20:11:23.010398Z",
     "shell.execute_reply.started": "2021-06-22T19:07:08.656347Z"
    },
    "papermill": {
     "duration": 0.086442,
     "end_time": "2021-06-22T20:11:23.010643",
     "exception": false,
     "start_time": "2021-06-22T20:11:22.924201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = Task('binary')\n",
    "roles = {\n",
    "    'target': TARGET_NAME,\n",
    "    'drop': ['SK_ID_CURR']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f8b1439",
   "metadata": {
    "papermill": {
     "duration": 0.074284,
     "end_time": "2021-06-22T20:11:23.582462",
     "exception": false,
     "start_time": "2021-06-22T20:11:23.508178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 LightAutoML model creation - TabularAutoML preset with neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56030975",
   "metadata": {
    "papermill": {
     "duration": 0.072649,
     "end_time": "2021-06-22T20:11:23.726154",
     "exception": false,
     "start_time": "2021-06-22T20:11:23.653505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In next the cell we are going to create LightAutoML model with `TabularAutoML` class.\n",
    "\n",
    "in just several lines. Let's discuss the params we can setup:\n",
    "- `task` - the type of the ML task (the only **must have** parameter)\n",
    "- `timeout` - time limit in seconds for model to train\n",
    "- `cpu_limit` - vCPU count for model to use\n",
    "- `nn_params` - network and training params, for example, `\"hidden_size\"`, `\"batch_size\"`, `\"lr\"`, etc.\n",
    "- `nn_pipeline_params` - data preprocessing params, which affect how data is fed to the model: use embeddings or target encoding for categorical columns, standard scalar or quantile transformer for numerical columns\n",
    "- `reader_params` - parameter change for Reader object inside preset, which works on the first step of data preparation: automatic feature typization, preliminary almost-constant features, correct CV setup etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf5d0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = TabularAutoML(\n",
    "    task = task, \n",
    "    timeout = TIMEOUT,\n",
    "    cpu_limit = N_THREADS,\n",
    "    general_params = {\"use_algos\": [[\"mlp\"]]}, # ['nn', 'mlp', 'dense', 'denselight', 'resnet', 'snn', 'node', 'autoint', 'fttransformer'] or custom torch model\n",
    "    nn_params = {\"n_epochs\": 10, \"bs\": 512, \"num_workers\": 0, \"path_to_save\": None, \"freeze_defaults\": True},\n",
    "    nn_pipeline_params = {\"use_qnt\": True, \"use_te\": False},\n",
    "    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "910ee822",
   "metadata": {},
   "source": [
    "### 2.3 AutoML training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da4245",
   "metadata": {},
   "source": [
    "To run autoML training use fit_predict method:\n",
    "\n",
    "- `train_data` - Dataset to train.\n",
    "- `roles` - Roles dict.\n",
    "- `verbose` - Controls the verbosity: the higher, the more messages.\n",
    "        <1  : messages are not displayed;\n",
    "        >=1 : the computation process for layers is displayed;\n",
    "        >=2 : the information about folds processing is also displayed;\n",
    "        >=3 : the hyperparameters optimization process is also displayed;\n",
    "        >=4 : the training process for every algorithm is displayed;\n",
    "\n",
    "Note: out-of-fold prediction is calculated during training and returned from the fit_predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ddc26e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:35:05] Stdout logging level is INFO.\n",
      "[11:35:05] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer\n",
      "[11:35:05] Task: binary\n",
      "\n",
      "[11:35:05] Start automl preset with listed constraints:\n",
      "[11:35:05] - time: 300.00 seconds\n",
      "[11:35:05] - CPU: 4 cores\n",
      "[11:35:05] - memory: 16 GB\n",
      "\n",
      "[11:35:05] \u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
      "\n",
      "[11:35:07] Layer \u001b[1m1\u001b[0m train process start. Time left 297.35 secs\n",
      "[11:35:08] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m ...\n",
      "[11:35:20] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m finished. score = \u001b[1m0.6951557493612979\u001b[0m\n",
      "[11:35:20] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_mlp_0\u001b[0m fitting and predicting completed\n",
      "[11:35:20] Time left 284.31 secs\n",
      "\n",
      "[11:35:20] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:35:20] \u001b[1mAutoml preset training completed in 15.70 seconds\u001b[0m\n",
      "\n",
      "[11:35:20] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_mlp_0) \n",
      "\n",
      "CPU times: user 15.8 s, sys: 1.58 s, total: 17.4 s\n",
      "Wall time: 15.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "oof_pred = automl.fit_predict(tr_data, roles = roles, verbose = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b0a58a",
   "metadata": {
    "papermill": {
     "duration": 0.145098,
     "end_time": "2021-06-22T20:34:32.530768",
     "exception": false,
     "start_time": "2021-06-22T20:34:32.385670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.4 Prediction on holdout and model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e2b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for te_data:\n",
      "array([[0.08216639],\n",
      "       [0.08314921],\n",
      "       [0.07000729],\n",
      "       ...,\n",
      "       [0.07061756],\n",
      "       [0.09196799],\n",
      "       [0.16275021]], dtype=float32)\n",
      "Shape = (2000, 1)\n",
      "CPU times: user 1.07 s, sys: 30.4 ms, total: 1.1 s\n",
      "Wall time: 1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "te_pred = automl.predict(te_data)\n",
    "print(f'Prediction for te_data:\\n{te_pred}\\nShape = {te_pred.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f93f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF score: 0.6951557493612979\n",
      "HOLDOUT score: 0.7132812500000001\n"
     ]
    }
   ],
   "source": [
    "print(f'OOF score: {roc_auc_score(tr_data[TARGET_NAME].values, oof_pred.data[:, 0])}')\n",
    "print(f'HOLDOUT score: {roc_auc_score(te_data[TARGET_NAME].values, te_pred.data[:, 0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9846b4",
   "metadata": {},
   "source": [
    "You can obtain the description of the resulting pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee6caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_mlp_0) \n"
     ]
    }
   ],
   "source": [
    "print(automl.create_model_str_desc())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41d07887",
   "metadata": {},
   "source": [
    "## 3. Main training loop and pipeline params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc28b650",
   "metadata": {},
   "source": [
    "### 3.1 Training loop params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b0633e5",
   "metadata": {},
   "source": [
    "<img src=\"../../docs/imgs/swa.png\" style=\"width:70%;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c7cd871",
   "metadata": {},
   "source": [
    "- `bs` - batch_size\n",
    "- `snap_params` - early stopping and checkpoint averaging params, stochastic weight averaging (swa)\n",
    "- `opt` - lr optimizer\n",
    "- `opt_params` - optimizer params\n",
    "- `clip_grad` - use grad clipping for regularization\n",
    "- `clip_grad_params`\n",
    "- `emb_dropout` - embedding dropout for categorical columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "401c164c",
   "metadata": {},
   "source": [
    "This set of params should be passed in `nn_params` as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc28b650",
   "metadata": {},
   "source": [
    "### 3.2 Pipeline params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "449bd024",
   "metadata": {},
   "source": [
    "Transformation for numerical columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09b79443",
   "metadata": {},
   "source": [
    "- `use_qnt` - uses quantile transformation for numerical columns\n",
    "- `output_distribution` - type of distribuiton of feature after qnt transformer\n",
    "- `n_quantiles` - number of quantiles used to build feature distribution\n",
    "- `qnt_factor` - decreses `n_quantiles` depending on train data shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "449bd024",
   "metadata": {},
   "source": [
    "Transformation for categorical columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a910e28",
   "metadata": {},
   "source": [
    "- `use_te` - uses target encoding\n",
    "- `top_intersections` - number of intersections of cat columns to use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79551da5",
   "metadata": {},
   "source": [
    "Full list of default parametres you can find here:\n",
    "- [nn_params](../../lightautoml/automl/presets/tabular_config.yml)\n",
    "- [nn_pipeline_params](../../lightautoml/automl/presets/tabular_config.yml)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c176647",
   "metadata": {},
   "source": [
    "## 4. More use cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c6863d3",
   "metadata": {},
   "source": [
    "Let's remember default Lama params to be more compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "343d7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lama_params = {\n",
    "    \"task\": task, \n",
    "    \"timeout\": TIMEOUT,\n",
    "    \"cpu_limit\": N_THREADS,\n",
    "    \"reader_params\": {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n",
    "}\n",
    "\n",
    "default_nn_params = {\n",
    "    \"bs\": 512, \"num_workers\": 0, \"path_to_save\": None, \"n_epochs\": 10, \"freeze_defaults\": True\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f82ac87",
   "metadata": {},
   "source": [
    "### 4.1 Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41519f",
   "metadata": {},
   "source": [
    "Consider simple neural network that we want to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32247a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in,\n",
    "        n_out,\n",
    "        hidden_size,\n",
    "        drop_rate,\n",
    "        **kwargs, # kwargs is must-have to hold unnecessary parameters\n",
    "    ):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.features = nn.Sequential(OrderedDict([]))\n",
    "\n",
    "        self.features.add_module(\"norm\", nn.BatchNorm1d(n_in))\n",
    "        self.features.add_module(\"dense1\", nn.Linear(n_in, hidden_size))\n",
    "        self.features.add_module(\"act\", nn.SiLU())\n",
    "        self.features.add_module(\"dropout\", nn.Dropout(p=drop_rate))\n",
    "        self.features.add_module(\"dense2\", nn.Linear(hidden_size, n_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: data after feature pipeline transformation\n",
    "            (by default concatenation of columns)\n",
    "        \"\"\"\n",
    "        for layer in self.features:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7e359df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:39:19] Stdout logging level is INFO.\n",
      "[11:39:19] Task: binary\n",
      "\n",
      "[11:39:19] Start automl preset with listed constraints:\n",
      "[11:39:19] - time: 300.00 seconds\n",
      "[11:39:19] - CPU: 4 cores\n",
      "[11:39:19] - memory: 16 GB\n",
      "\n",
      "[11:39:19] \u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
      "\n",
      "[11:39:20] Layer \u001b[1m1\u001b[0m train process start. Time left 299.14 secs\n",
      "[11:39:20] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_0\u001b[0m ...\n",
      "[11:39:29] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_0\u001b[0m finished. score = \u001b[1m0.7060418025974987\u001b[0m\n",
      "[11:39:29] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_0\u001b[0m fitting and predicting completed\n",
      "[11:39:29] Time left 290.88 secs\n",
      "\n",
      "[11:39:29] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:39:29] \u001b[1mAutoml preset training completed in 9.12 seconds\u001b[0m\n",
      "\n",
      "[11:39:29] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_0) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02449569],\n",
       "       [0.03754642],\n",
       "       [0.04070117],\n",
       "       ...,\n",
       "       [0.06268083],\n",
       "       [0.19106267],\n",
       "       [0.13282676]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = TabularAutoML(\n",
    "    **default_lama_params,\n",
    "    general_params={\"use_algos\": [[SimpleNet]]},\n",
    "    nn_params={\n",
    "        **default_nn_params,\n",
    "        \"hidden_size\": 256,\n",
    "        \"drop_rate\": 0.1\n",
    "    },\n",
    ")\n",
    "automl.fit_predict(tr_data, roles=roles, verbose=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbf8a589",
   "metadata": {},
   "source": [
    "#### 4.1.1 Define the pipeline by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7015726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from typing import Any\n",
    "from typing import Callable\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class CatEmbedder(nn.Module):\n",
    "    \"\"\"Category data model.\n",
    "\n",
    "    Args:\n",
    "        cat_dims: Sequence with number of unique categories\n",
    "            for category features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_dims: Sequence[int],\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(CatEmbedder, self).__init__()\n",
    "        emb_dims = [\n",
    "            (int(x), 5)\n",
    "            for x in cat_dims\n",
    "        ]\n",
    "        self.no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n",
    "    \n",
    "    def get_out_shape(self) -> int:\n",
    "        \"\"\"Output shape.\n",
    "\n",
    "        Returns:\n",
    "            Int with module output shape.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.no_of_embs\n",
    "\n",
    "    def forward(self, inp: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Concat all categorical embeddings\n",
    "        \"\"\"\n",
    "        output = torch.cat(\n",
    "            [\n",
    "                emb_layer(inp[\"cat\"][:, i])\n",
    "                for i, emb_layer in enumerate(self.emb_layers)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "        return output\n",
    "\n",
    "\n",
    "class ContEmbedder(nn.Module):\n",
    "    \"\"\"Numeric data model.\n",
    "\n",
    "    Class for working with numeric data.\n",
    "\n",
    "    Args:\n",
    "        num_dims: Sequence with number of numeric features.\n",
    "        input_bn: Use 1d batch norm for input data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_dims: int,  **kwargs):\n",
    "        super(ContEmbedder, self).__init__()\n",
    "        self.n_out = num_dims\n",
    "    \n",
    "    def get_out_shape(self) -> int:\n",
    "        \"\"\"Output shape.\n",
    "\n",
    "        Returns:\n",
    "            int with module output shape.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.n_out\n",
    "        \n",
    "    def forward(self, inp: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Forward-pass.\"\"\"\n",
    "        return (inp[\"cont\"] - inp[\"cont\"].mean(axis=0)) / (inp[\"cont\"].std(axis=0) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "998ea71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightautoml.text.nn_model import TorchUniversalModel\n",
    "\n",
    "class SimpleNet_plus(TorchUniversalModel):\n",
    "    \"\"\"Mixed data model.\n",
    "\n",
    "    Class for preparing input for DL model with mixed data.\n",
    "\n",
    "    Args:\n",
    "            n_out: Number of output dimensions.\n",
    "            cont_params: Dict with numeric model params.\n",
    "            cat_params: Dict with category model para\n",
    "            **kwargs: Loss, task and other parameters.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_out: int = 1,\n",
    "            cont_params: Optional[Dict] = None,\n",
    "            cat_params: Optional[Dict] = None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        # init parent class (need some helper functions to be used)\n",
    "        super(SimpleNet_plus, self).__init__(**{\n",
    "                **kwargs,\n",
    "                \"cont_params\": cont_params,\n",
    "                \"cat_params\": cat_params,\n",
    "                \"torch_model\": None, # dont need any model inside parent class\n",
    "        })\n",
    "        \n",
    "        n_in = 0\n",
    "        \n",
    "        # add cont columns processing\n",
    "        self.cont_embedder = ContEmbedder(**cont_params)\n",
    "        n_in += self.cont_embedder.get_out_shape()\n",
    "        \n",
    "        # add cat columns processing\n",
    "        self.cat_embedder = CatEmbedder(**cat_params)\n",
    "        n_in += self.cat_embedder.get_out_shape()\n",
    "        \n",
    "        self.torch_model = SimpleNet(\n",
    "                **{\n",
    "                    **kwargs,\n",
    "                    **{\"n_in\": n_in, \"n_out\": n_out},\n",
    "                }\n",
    "        )\n",
    "    \n",
    "    def get_logits(self, inp: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        outputs = []\n",
    "        outputs.append(self.cont_embedder(inp))\n",
    "        outputs.append(self.cat_embedder(inp))\n",
    "        \n",
    "        if len(outputs) > 1:\n",
    "            output = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            output = outputs[0]\n",
    "        \n",
    "        logits = self.torch_model(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e0fd192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:39:33] Stdout logging level is INFO.\n",
      "[11:39:33] Task: binary\n",
      "\n",
      "[11:39:33] Start automl preset with listed constraints:\n",
      "[11:39:33] - time: 300.00 seconds\n",
      "[11:39:33] - CPU: 4 cores\n",
      "[11:39:33] - memory: 16 GB\n",
      "\n",
      "[11:39:33] \u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
      "\n",
      "[11:39:34] Layer \u001b[1m1\u001b[0m train process start. Time left 299.14 secs\n",
      "[11:39:34] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_0\u001b[0m ...\n",
      "[11:39:42] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_0\u001b[0m finished. score = \u001b[1m0.680797945608108\u001b[0m\n",
      "[11:39:42] \u001b[1mLvl_0_Pipe_0_Mod_0_TorchNN_0\u001b[0m fitting and predicting completed\n",
      "[11:39:42] Time left 290.91 secs\n",
      "\n",
      "[11:39:42] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:39:42] \u001b[1mAutoml preset training completed in 9.10 seconds\u001b[0m\n",
      "\n",
      "[11:39:42] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_TorchNN_0) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06662331],\n",
       "       [0.05009553],\n",
       "       [0.05109952],\n",
       "       ...,\n",
       "       [0.07657926],\n",
       "       [0.19059831],\n",
       "       [0.04237348]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = TabularAutoML(\n",
    "    **default_lama_params,\n",
    "    general_params={\"use_algos\": [[SimpleNet_plus]]},\n",
    "    nn_params={\n",
    "        **default_nn_params,\n",
    "        \"hidden_size\": 256,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"model_with_emb\": True,\n",
    "    },\n",
    "    debug=True\n",
    ")\n",
    "automl.fit_predict(tr_data, roles = roles, verbose = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f82ac87",
   "metadata": {},
   "source": [
    "### 4.2 Tuning network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e42c7ac0",
   "metadata": {},
   "source": [
    "One can try optimize metric with the help of Optuna. Among validation stratagies there are:\n",
    "- `fit_on_holdout = True` - holdout\n",
    "- `fit_on_holdout = False` - cross-validation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f82ac87",
   "metadata": {},
   "source": [
    "#### 4.2.1 Built-in models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78dfb657",
   "metadata": {},
   "source": [
    "Use `\"_tuned\"` in model name to tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "668a5dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:41:13] Stdout logging level is INFO3.\n",
      "[11:41:13] Task: binary\n",
      "\n",
      "[11:41:13] Start automl preset with listed constraints:\n",
      "[11:41:13] - time: 300.00 seconds\n",
      "[11:41:13] - CPU: 4 cores\n",
      "[11:41:13] - memory: 16 GB\n",
      "\n",
      "[11:41:13] \u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
      "\n",
      "[11:41:14] Feats was rejected during automatic roles guess: []\n",
      "[11:41:14] Layer \u001b[1m1\u001b[0m train process start. Time left 299.15 secs\n",
      "[11:41:14] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m ... Time budget is 100.00 secs\n",
      "[11:41:15] Epoch: 0, train loss: 1.0535998344421387, val loss: 0.32914862036705017, val metric: 0.6417082284266402\n",
      "[11:41:16] Epoch: 1, train loss: 0.2719154357910156, val loss: 0.29687464237213135, val metric: 0.7061383111225151\n",
      "[11:41:16] Epoch: 2, train loss: 0.2606324255466461, val loss: 0.26732537150382996, val metric: 0.7064643905255223\n",
      "[11:41:16] Early stopping: val loss: 0.27287718653678894, val metric: 0.7066167390990586\n",
      "[11:41:16] \u001b[1mTrial 1\u001b[0m with hyperparameters {'bs': 128, 'weight_decay_bin': 0, 'lr': 0.029154431891537533} scored 0.7066167390990586 in 0:00:01.938430\n",
      "[11:41:17] Epoch: 0, train loss: 0.28143310546875, val loss: 0.30991676449775696, val metric: 0.6123530638099972\n",
      "[11:41:17] Epoch: 1, train loss: 0.27844926714897156, val loss: 0.3095921576023102, val metric: 0.6240812311902967\n",
      "[11:41:17] Epoch: 2, train loss: 0.27550050616264343, val loss: 0.3089315891265869, val metric: 0.629087351861058\n",
      "[11:41:17] Early stopping: val loss: 0.3095770478248596, val metric: 0.6233248338865992\n",
      "[11:41:17] \u001b[1mTrial 2\u001b[0m with hyperparameters {'bs': 512, 'weight_decay_bin': 0, 'lr': 5.415244119402538e-05} scored 0.6233248338865992 in 0:00:00.755362\n",
      "[11:41:17] Epoch: 0, train loss: 0.2861214578151703, val loss: 0.2767995595932007, val metric: 0.6338181759866575\n",
      "[11:41:18] Epoch: 1, train loss: 0.2688417136669159, val loss: 0.26842930912971497, val metric: 0.6981253107109064\n",
      "[11:41:18] Epoch: 2, train loss: 0.24808287620544434, val loss: 0.2549731731414795, val metric: 0.7318772017041658\n",
      "[11:41:18] Early stopping: val loss: 0.2691458761692047, val metric: 0.7062398768382059\n",
      "[11:41:18] \u001b[1mTrial 3\u001b[0m with hyperparameters {'bs': 1024, 'weight_decay_bin': 1, 'weight_decay': 2.9204338471814107e-05, 'lr': 0.0006672367170464204} scored 0.7062398768382059 in 0:00:00.636955\n",
      "[11:41:19] Epoch: 0, train loss: 0.2786088287830353, val loss: 0.27673330903053284, val metric: 0.6143309224839767\n",
      "[11:41:20] Epoch: 1, train loss: 0.2777416706085205, val loss: 0.27581408619880676, val metric: 0.6298918592406093\n",
      "[11:41:21] Epoch: 2, train loss: 0.27608099579811096, val loss: 0.2738899886608124, val metric: 0.6364695757225867\n",
      "[11:41:21] Early stopping: val loss: 0.2757117748260498, val metric: 0.6301484463118281\n",
      "[11:41:21] \u001b[1mTrial 4\u001b[0m with hyperparameters {'bs': 64, 'weight_decay_bin': 0, 'lr': 1.8205657658407255e-05} scored 0.6301484463118281 in 0:00:03.412576\n",
      "[11:41:22] Epoch: 0, train loss: 0.27859607338905334, val loss: 0.2825257182121277, val metric: 0.6120082749330469\n",
      "[11:41:22] Epoch: 1, train loss: 0.27754485607147217, val loss: 0.2815532982349396, val metric: 0.6283122450834175\n",
      "[11:41:23] Epoch: 2, train loss: 0.27565139532089233, val loss: 0.2794336676597595, val metric: 0.6365551047463264\n",
      "[11:41:23] Early stopping: val loss: 0.28141137957572937, val metric: 0.6296112171314634\n",
      "[11:41:23] \u001b[1mTrial 5\u001b[0m with hyperparameters {'bs': 128, 'weight_decay_bin': 0, 'lr': 3.077180271250682e-05} scored 0.6296112171314634 in 0:00:01.886276\n",
      "[11:41:23] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m completed\n",
      "[11:41:23] The set of hyperparameters \u001b[1m{'num_workers': 0, 'pin_memory': False, 'max_length': 256, 'is_snap': False, 'input_bn': False, 'max_emb_size': 256, 'bert_name': None, 'pooling': 'cls', 'device': ['0', '1'], 'use_cont': True, 'use_cat': True, 'use_text': False, 'lang': 'en', 'deterministic': True, 'multigpu': False, 'random_state': 42, 'model': 'denselight', 'model_with_emb': False, 'path_to_save': None, 'verbose_inside': None, 'verbose': 1, 'n_epochs': 3, 'snap_params': {'k': 3, 'early_stopping': True, 'patience': 10, 'swa': True}, 'bs': 128, 'emb_dropout': 0.1, 'emb_ratio': 3, 'opt': 'Adam', 'opt_params': {'lr': 0.029154431891537533, 'weight_decay': 0}, 'sch': 'ReduceLROnPlateau', 'scheduler_params': {'patience': 5, 'factor': 0.5, 'min_lr': 1e-05}, 'loss': None, 'loss_params': {}, 'loss_on_logits': True, 'clip_grad': False, 'clip_grad_params': {}, 'init_bias': True, 'dataset': 'UniversalDataset', 'tuned': False, 'optimization_search_space': None, 'verbose_bar': False, 'freeze_defaults': True, 'n_out': None, 'hid_factor': [2, 2], 'hidden_size': [512, 512, 512], 'block_config': [2, 2], 'compression': 0.5, 'growth_size': 256, 'bn_factor': 2, 'drop_rate': 0.1, 'noise_std': 0.05, 'num_init_features': None, 'act_fun': 'ReLU', 'use_noise': False, 'use_bn': True, 'stop_by_metric': False, 'tuning_params': {'fit_on_holdout': True, 'max_tuning_iter': 5, 'max_tuning_time': 100}}\u001b[0m\n",
      " achieve 0.7066 auc\n",
      "[11:41:23] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m ...\n",
      "[11:41:23] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m =====\n",
      "[11:41:24] Epoch: 0, train loss: 1.0535998344421387, val loss: 0.32914862036705017, val metric: 0.6417082284266402\n",
      "[11:41:24] Epoch: 1, train loss: 0.2719154357910156, val loss: 0.29687464237213135, val metric: 0.7061383111225151\n",
      "[11:41:25] Epoch: 2, train loss: 0.2606324255466461, val loss: 0.26732537150382996, val metric: 0.7064643905255223\n",
      "[11:41:25] Early stopping: val loss: 0.27287718653678894, val metric: 0.7066167390990586\n",
      "[11:41:25] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m =====\n",
      "[11:41:26] Epoch: 0, train loss: 1.9826449155807495, val loss: 0.2658182680606842, val metric: 0.6983562967051631\n",
      "[11:41:26] Epoch: 1, train loss: 0.27176782488822937, val loss: 0.2550528347492218, val metric: 0.7206712805706522\n",
      "[11:41:27] Epoch: 2, train loss: 0.25288328528404236, val loss: 0.25439774990081787, val metric: 0.7291525135869564\n",
      "[11:41:27] Early stopping: val loss: 0.25335100293159485, val metric: 0.7309198794157609\n",
      "[11:41:27] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m =====\n",
      "[11:41:27] Epoch: 0, train loss: 2.025099277496338, val loss: 0.3039107024669647, val metric: 0.5462911854619565\n",
      "[11:41:28] Epoch: 1, train loss: 0.2893942892551422, val loss: 0.28194475173950195, val metric: 0.648352581521739\n",
      "[11:41:28] Epoch: 2, train loss: 0.25209811329841614, val loss: 0.2732546031475067, val metric: 0.6559634001358696\n",
      "[11:41:29] Early stopping: val loss: 0.2753060460090637, val metric: 0.6604216202445652\n",
      "[11:41:29] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m =====\n",
      "[11:41:29] Epoch: 0, train loss: 1.485915184020996, val loss: 0.3707677721977234, val metric: 0.590682319972826\n",
      "[11:41:30] Epoch: 1, train loss: 0.27479660511016846, val loss: 0.283542662858963, val metric: 0.6689877717391306\n",
      "[11:41:30] Epoch: 2, train loss: 0.27139508724212646, val loss: 0.2609306275844574, val metric: 0.7076362941576086\n",
      "[11:41:30] Early stopping: val loss: 0.26281988620758057, val metric: 0.6873938519021738\n",
      "[11:41:31] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m =====\n",
      "[11:41:31] Epoch: 0, train loss: 1.2433826923370361, val loss: 0.2741888165473938, val metric: 0.6601270592730978\n",
      "[11:41:32] Epoch: 1, train loss: 0.28257879614830017, val loss: 0.2676794230937958, val metric: 0.6493397588315217\n",
      "[11:41:32] Epoch: 2, train loss: 0.26426026225090027, val loss: 0.2751479148864746, val metric: 0.6779360563858697\n",
      "[11:41:32] Early stopping: val loss: 0.2678437829017639, val metric: 0.656356148097826\n",
      "[11:41:32] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m finished. score = \u001b[1m0.682772889051315\u001b[0m\n",
      "[11:41:32] \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0\u001b[0m fitting and predicting completed\n",
      "[11:41:32] Time left 280.97 secs\n",
      "\n",
      "[11:41:32] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:41:32] \u001b[1mAutoml preset training completed in 19.03 seconds\u001b[0m\n",
      "\n",
      "[11:41:32] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_Tuned_TorchNN_denselight_tuned_0) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00909923],\n",
       "       [0.06779448],\n",
       "       [0.05014049],\n",
       "       ...,\n",
       "       [0.04888163],\n",
       "       [0.18241519],\n",
       "       [0.07331596]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = TabularAutoML(\n",
    "    **default_lama_params,\n",
    "    general_params={\"use_algos\": [[\"denselight_tuned\"]]},\n",
    "    nn_params={\n",
    "        **default_nn_params,\n",
    "        \"n_epochs\": 3,\n",
    "        \"tuning_params\": {\n",
    "            \"max_tuning_iter\": 5,\n",
    "            \"max_tuning_time\": 100,\n",
    "            \"fit_on_holdout\": True\n",
    "        }\n",
    "    },\n",
    ")\n",
    "automl.fit_predict(tr_data, roles = roles, verbose = 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f82ac87",
   "metadata": {},
   "source": [
    "#### 4.2.2 Custom model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78dfb657",
   "metadata": {},
   "source": [
    "There is a spesial flag `tuned` to mark that you need optimize parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "668a5dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:41:56] Stdout logging level is INFO2.\n",
      "[11:41:56] Task: binary\n",
      "\n",
      "[11:41:56] Start automl preset with listed constraints:\n",
      "[11:41:56] - time: 300.00 seconds\n",
      "[11:41:56] - CPU: 4 cores\n",
      "[11:41:56] - memory: 16 GB\n",
      "\n",
      "[11:41:56] \u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
      "\n",
      "[11:41:57] Layer \u001b[1m1\u001b[0m train process start. Time left 299.16 secs\n",
      "[11:41:57] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m ... Time budget is 100.00 secs\n",
      "[11:42:17] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m completed\n",
      "[11:42:17] The set of hyperparameters \u001b[1m{'num_workers': 0, 'pin_memory': False, 'max_length': 256, 'is_snap': False, 'input_bn': False, 'max_emb_size': 256, 'bert_name': None, 'pooling': 'cls', 'device': ['0', '1'], 'use_cont': True, 'use_cat': True, 'use_text': False, 'lang': 'en', 'deterministic': True, 'multigpu': False, 'random_state': 42, 'model': <class '__main__.SimpleNet'>, 'model_with_emb': False, 'path_to_save': None, 'verbose_inside': None, 'verbose': 1, 'n_epochs': 10, 'snap_params': {'k': 3, 'early_stopping': True, 'patience': 10, 'swa': True}, 'bs': 128, 'emb_dropout': 0.1, 'emb_ratio': 3, 'opt': 'Adam', 'opt_params': {'lr': 0.029154431891537533, 'weight_decay': 0}, 'sch': 'ReduceLROnPlateau', 'scheduler_params': {'patience': 5, 'factor': 0.5, 'min_lr': 1e-05}, 'loss': None, 'loss_params': {}, 'loss_on_logits': True, 'clip_grad': False, 'clip_grad_params': {}, 'init_bias': True, 'dataset': 'UniversalDataset', 'tuned': True, 'optimization_search_space': None, 'verbose_bar': False, 'freeze_defaults': True, 'n_out': None, 'hid_factor': [2, 2], 'hidden_size': 256, 'block_config': [2, 2], 'compression': 0.5, 'growth_size': 256, 'bn_factor': 2, 'drop_rate': 0.1, 'noise_std': 0.05, 'num_init_features': None, 'act_fun': 'ReLU', 'use_noise': False, 'use_bn': True, 'stop_by_metric': False, 'tuning_params': {'fit_on_holdout': True, 'max_tuning_iter': 5, 'max_tuning_time': 100}}\u001b[0m\n",
      " achieve 0.7581 auc\n",
      "[11:42:17] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m ...\n",
      "[11:42:17] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:21] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:26] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:30] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:34] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:38] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m finished. score = \u001b[1m0.7200461596125074\u001b[0m\n",
      "[11:42:38] \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m fitting and predicting completed\n",
      "[11:42:38] Time left 258.19 secs\n",
      "\n",
      "[11:42:38] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:42:38] \u001b[1mAutoml preset training completed in 41.82 seconds\u001b[0m\n",
      "\n",
      "[11:42:38] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01359685],\n",
       "       [0.02897443],\n",
       "       [0.01692689],\n",
       "       ...,\n",
       "       [0.04529661],\n",
       "       [0.17770922],\n",
       "       [0.17924136]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = TabularAutoML(\n",
    "    **default_lama_params,\n",
    "    general_params={\"use_algos\": [[SimpleNet]]},\n",
    "    nn_params={\n",
    "        **default_nn_params,\n",
    "        \"hidden_size\": 256,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \n",
    "        \"tuned\": True,\n",
    "        \"tuning_params\": {\n",
    "            \"max_tuning_iter\": 5,\n",
    "            \"max_tuning_time\": 100,\n",
    "            \"fit_on_holdout\": True\n",
    "        }\n",
    "    },\n",
    ")\n",
    "automl.fit_predict(tr_data, roles = roles, verbose = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f52aa5e2",
   "metadata": {},
   "source": [
    "Sometimes we need to tune parameters that we define by ourself. To this purpose we have `optimization_search_space` which describes necessary parameter grid. See example below.  \n",
    "Here is the grid:  \n",
    "- `bs` in `[64, 128, 256, 512, 1024]`\n",
    "- `hidden_size` in `[64, 128, 256, 512, 1024]`\n",
    "- `drop_rate` in `[0.0, 0.3]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e905c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_opt_space(trial: optuna.trial.Trial, estimated_n_trials, suggested_params):\n",
    "    ''' \n",
    "        This function needs for parameter tuning\n",
    "    '''\n",
    "    # optionally\n",
    "    trial_values = copy(suggested_params)\n",
    "\n",
    "    trial_values[\"bs\"] = trial.suggest_categorical(\n",
    "        \"bs\", [2 ** i for i in range(6, 11)]\n",
    "    )\n",
    "    trial_values[\"hidden_size\"] = trial.suggest_categorical(\n",
    "        \"hidden_size\", [2 ** i for i in range(6, 11)]\n",
    "    )\n",
    "    trial_values[\"drop_rate\"] = trial.suggest_float(\n",
    "        \"drop_rate\", 0.0, 0.3\n",
    "    )\n",
    "    return trial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2398295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:42:39] Stdout logging level is INFO3.\n",
      "[11:42:39] Task: binary\n",
      "\n",
      "[11:42:39] Start automl preset with listed constraints:\n",
      "[11:42:39] - time: 300.00 seconds\n",
      "[11:42:39] - CPU: 4 cores\n",
      "[11:42:39] - memory: 16 GB\n",
      "\n",
      "[11:42:39] \u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
      "\n",
      "[11:42:39] Feats was rejected during automatic roles guess: []\n",
      "[11:42:40] Layer \u001b[1m1\u001b[0m train process start. Time left 299.16 secs\n",
      "[11:42:40] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m ... Time budget is 156.93 secs\n",
      "[11:42:40] Epoch: 0, train loss: 0.2768203020095825, val loss: 0.27835753560066223, val metric: 0.6358815636843766\n",
      "[11:42:41] Epoch: 1, train loss: 0.269419401884079, val loss: 0.270462304353714, val metric: 0.6819656707880963\n",
      "[11:42:41] Epoch: 2, train loss: 0.2603665292263031, val loss: 0.26111170649528503, val metric: 0.7290280161008387\n",
      "[11:42:41] Early stopping: val loss: 0.2704150676727295, val metric: 0.6917587440062863\n",
      "[11:42:41] \u001b[1mTrial 1\u001b[0m with hyperparameters {'bs': 128, 'hidden_size': 256, 'drop_rate': 0.006175348288740734} scored 0.6917587440062863 in 0:00:01.446466\n",
      "[11:42:42] Epoch: 0, train loss: 0.27193132042884827, val loss: 0.2582079768180847, val metric: 0.7192162334087058\n",
      "[11:42:43] Epoch: 1, train loss: 0.25488120317459106, val loss: 0.2459825575351715, val metric: 0.763859711018811\n",
      "[11:42:43] Epoch: 2, train loss: 0.2454848736524582, val loss: 0.24529244005680084, val metric: 0.7586050216228063\n",
      "[11:42:44] Early stopping: val loss: 0.24799665808677673, val metric: 0.7566271629488269\n",
      "[11:42:44] \u001b[1mTrial 2\u001b[0m with hyperparameters {'bs': 64, 'hidden_size': 1024, 'drop_rate': 0.04184815819561255} scored 0.7566271629488269 in 0:00:02.566996\n",
      "[11:42:44] Epoch: 0, train loss: 0.2784821093082428, val loss: 0.30797627568244934, val metric: 0.6284539025289864\n",
      "[11:42:44] Epoch: 1, train loss: 0.27420976758003235, val loss: 0.30563732981681824, val metric: 0.6298303852547963\n",
      "[11:42:44] Epoch: 2, train loss: 0.26723629236221313, val loss: 0.3022131323814392, val metric: 0.6563443826140877\n",
      "[11:42:44] Early stopping: val loss: 0.30519333481788635, val metric: 0.6395967306530677\n",
      "[11:42:44] \u001b[1mTrial 3\u001b[0m with hyperparameters {'bs': 512, 'hidden_size': 512, 'drop_rate': 0.019515477895583853} scored 0.6395967306530677 in 0:00:00.620905\n",
      "[11:42:45] Epoch: 0, train loss: 0.2780534625053406, val loss: 0.2811879515647888, val metric: 0.6194653366903475\n",
      "[11:42:45] Epoch: 1, train loss: 0.2749352753162384, val loss: 0.27780336141586304, val metric: 0.6365337224903913\n",
      "[11:42:46] Epoch: 2, train loss: 0.2703269124031067, val loss: 0.2734648585319519, val metric: 0.6630851387975689\n",
      "[11:42:46] Early stopping: val loss: 0.27777808904647827, val metric: 0.641825830834282\n",
      "[11:42:46] \u001b[1mTrial 4\u001b[0m with hyperparameters {'bs': 128, 'hidden_size': 64, 'drop_rate': 0.2727961206236346} scored 0.641825830834282 in 0:00:01.440003\n",
      "[11:42:46] Epoch: 0, train loss: 0.27747318148612976, val loss: 0.2802681624889374, val metric: 0.6187169577326256\n",
      "[11:42:47] Epoch: 1, train loss: 0.27283716201782227, val loss: 0.27530962228775024, val metric: 0.6516670141283256\n",
      "[11:42:47] Epoch: 2, train loss: 0.2660568654537201, val loss: 0.268929660320282, val metric: 0.6927262910873412\n",
      "[11:42:47] Early stopping: val loss: 0.2752225697040558, val metric: 0.6592897883691219\n",
      "[11:42:47] \u001b[1mTrial 5\u001b[0m with hyperparameters {'bs': 128, 'hidden_size': 128, 'drop_rate': 0.17936999364332554} scored 0.6592897883691219 in 0:00:01.443614\n",
      "[11:42:47] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m completed\n",
      "[11:42:47] The set of hyperparameters \u001b[1m{'num_workers': 0, 'pin_memory': False, 'max_length': 256, 'is_snap': False, 'input_bn': False, 'max_emb_size': 256, 'bert_name': None, 'pooling': 'cls', 'device': ['0', '1'], 'use_cont': True, 'use_cat': True, 'use_text': False, 'lang': 'en', 'deterministic': True, 'multigpu': False, 'random_state': 42, 'model': <class '__main__.SimpleNet'>, 'model_with_emb': False, 'path_to_save': None, 'verbose_inside': None, 'verbose': 1, 'n_epochs': 3, 'snap_params': {'k': 3, 'early_stopping': True, 'patience': 10, 'swa': True}, 'bs': 64, 'emb_dropout': 0.1, 'emb_ratio': 3, 'opt': 'Adam', 'opt_params': {'lr': 0.0003, 'weight_decay': 0}, 'sch': 'ReduceLROnPlateau', 'scheduler_params': {'patience': 5, 'factor': 0.5, 'min_lr': 1e-05}, 'loss': None, 'loss_params': {}, 'loss_on_logits': True, 'clip_grad': False, 'clip_grad_params': {}, 'init_bias': True, 'dataset': 'UniversalDataset', 'tuned': True, 'optimization_search_space': <function my_opt_space at 0x7f4dc0a14790>, 'verbose_bar': False, 'freeze_defaults': True, 'n_out': None, 'hid_factor': [2, 2], 'hidden_size': 1024, 'block_config': [2, 2], 'compression': 0.5, 'growth_size': 256, 'bn_factor': 2, 'drop_rate': 0.04184815819561255, 'noise_std': 0.05, 'num_init_features': None, 'act_fun': 'ReLU', 'use_noise': False, 'use_bn': True, 'stop_by_metric': False, 'tuning_params': {'fit_on_holdout': True, 'max_tuning_iter': 5, 'max_tuning_time': 3600}}\u001b[0m\n",
      " achieve 0.7566 auc\n",
      "[11:42:47] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m ...\n",
      "[11:42:47] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:48] Epoch: 0, train loss: 0.27193132042884827, val loss: 0.2582079768180847, val metric: 0.7192162334087058\n",
      "[11:42:49] Epoch: 1, train loss: 0.25488120317459106, val loss: 0.2459825575351715, val metric: 0.763859711018811\n",
      "[11:42:50] Epoch: 2, train loss: 0.2454848736524582, val loss: 0.24529244005680084, val metric: 0.7586050216228063\n",
      "[11:42:50] Early stopping: val loss: 0.24799665808677673, val metric: 0.7566271629488269\n",
      "[11:42:50] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:51] Epoch: 0, train loss: 0.27125343680381775, val loss: 0.26051169633865356, val metric: 0.7223749575407609\n",
      "[11:42:51] Epoch: 1, train loss: 0.254526287317276, val loss: 0.25250861048698425, val metric: 0.7373842985733696\n",
      "[11:42:52] Epoch: 2, train loss: 0.24166874587535858, val loss: 0.258941113948822, val metric: 0.7197000254755436\n",
      "[11:42:52] Early stopping: val loss: 0.2543417811393738, val metric: 0.7311799422554348\n",
      "[11:42:52] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:53] Epoch: 0, train loss: 0.2697919011116028, val loss: 0.26959753036499023, val metric: 0.6485861073369565\n",
      "[11:42:54] Epoch: 1, train loss: 0.2507198452949524, val loss: 0.26720497012138367, val metric: 0.6837476647418478\n",
      "[11:42:55] Epoch: 2, train loss: 0.24044080078601837, val loss: 0.2683129906654358, val metric: 0.6879935886548914\n",
      "[11:42:55] Early stopping: val loss: 0.26637837290763855, val metric: 0.678169582201087\n",
      "[11:42:55] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:56] Epoch: 0, train loss: 0.27104493975639343, val loss: 0.26356974244117737, val metric: 0.6876220703125\n",
      "[11:42:56] Epoch: 1, train loss: 0.2532782256603241, val loss: 0.2557208836078644, val metric: 0.7101042374320653\n",
      "[11:42:57] Epoch: 2, train loss: 0.2443435788154602, val loss: 0.25609511137008667, val metric: 0.7170038637907609\n",
      "[11:42:57] Early stopping: val loss: 0.2564053535461426, val metric: 0.7097698709239131\n",
      "[11:42:57] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m =====\n",
      "[11:42:58] Epoch: 0, train loss: 0.2717946171760559, val loss: 0.26010674238204956, val metric: 0.7114735478940217\n",
      "[11:42:59] Epoch: 1, train loss: 0.25437426567077637, val loss: 0.250105619430542, val metric: 0.7436258067255435\n",
      "[11:43:00] Epoch: 2, train loss: 0.24573101103305817, val loss: 0.24898070096969604, val metric: 0.7468049422554348\n",
      "[11:43:00] Early stopping: val loss: 0.251451313495636, val metric: 0.7411684782608696\n",
      "[11:43:00] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m finished. score = \u001b[1m0.7228784107078735\u001b[0m\n",
      "[11:43:00] \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0\u001b[0m fitting and predicting completed\n",
      "[11:43:00] Time left 278.70 secs\n",
      "\n",
      "[11:43:00] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:43:00] \u001b[1mAutoml preset training completed in 21.30 seconds\u001b[0m\n",
      "\n",
      "[11:43:00] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_Tuned_TorchNN_0) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03668038],\n",
       "       [0.03266167],\n",
       "       [0.0428489 ],\n",
       "       ...,\n",
       "       [0.05952494],\n",
       "       [0.19782786],\n",
       "       [0.10605511]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = TabularAutoML(\n",
    "    **default_lama_params,\n",
    "    general_params={\"use_algos\": [[SimpleNet]]},\n",
    "    nn_params={\n",
    "        **default_nn_params,\n",
    "        \"n_epochs\": 3,\n",
    "        \"tuned\": True,\n",
    "        \"tuning_params\": {\n",
    "            \"max_tuning_iter\": 5,\n",
    "            \"max_tuning_time\": 3600,\n",
    "            \"fit_on_holdout\": True\n",
    "        },\n",
    "        \"optimization_search_space\": my_opt_space,\n",
    "    },\n",
    ")\n",
    "automl.fit_predict(tr_data, roles = roles, verbose = 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1000351d",
   "metadata": {},
   "source": [
    "##### 4.2.3 One more example\n",
    "##### Tuning NODE params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbad7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUT = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3bba8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lama_params = {\n",
    "    \"task\": task, \n",
    "    \"timeout\": TIMEOUT,\n",
    "    \"cpu_limit\": N_THREADS,\n",
    "    \"reader_params\": {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n",
    "}\n",
    "\n",
    "default_nn_params = {\n",
    "    \"bs\": 512, \"num_workers\": 0, \"path_to_save\": None, \"n_epochs\": 10, \"freeze_defaults\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec77132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_opt_space_NODE(trial: optuna.trial.Trial, estimated_n_trials, suggested_params):\n",
    "    ''' \n",
    "        This function needs for parameter tuning\n",
    "    '''\n",
    "    # optionally\n",
    "    trial_values = copy(suggested_params)\n",
    "\n",
    "    trial_values[\"layer_dim\"] = trial.suggest_categorical(\n",
    "        \"layer_dim\", [2 ** i for i in range(8, 10)]\n",
    "    )\n",
    "    trial_values[\"use_original_head\"] = trial.suggest_categorical(\n",
    "        \"use_original_head\", [True, False]\n",
    "    )\n",
    "    trial_values[\"num_layers\"] = trial.suggest_int(\n",
    "        \"num_layers\", 1, 3\n",
    "    )\n",
    "    trial_values[\"drop_rate\"] = trial.suggest_float(\n",
    "        \"drop_rate\", 0.0, 0.3\n",
    "    )\n",
    "    trial_values[\"tree_dim\"] = trial.suggest_int(\n",
    "        \"tree_dim\", 1, 3\n",
    "    )\n",
    "    return trial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba312d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = TabularAutoML(\n",
    "    task = task, \n",
    "    timeout = TIMEOUT,\n",
    "    cpu_limit = N_THREADS,\n",
    "    general_params = {\"use_algos\": [[\"node_tuned\"]]}, # ['nn', 'mlp', 'dense', 'denselight', 'resnet', 'snn'] or custom torch model\n",
    "    nn_params = {\"n_epochs\": 10, \"bs\": 512, \"num_workers\": 0, \"path_to_save\": None, \"freeze_defaults\": True, \"optimization_search_space\": my_opt_space_NODE,},\n",
    "    nn_pipeline_params = {\"use_qnt\": True, \"use_te\": False},\n",
    "    reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3df2104f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:58:03] Stdout logging level is INFO2.\n",
      "[11:58:03] Task: binary\n",
      "\n",
      "[11:58:03] Start automl preset with listed constraints:\n",
      "[11:58:03] - time: 3000.00 seconds\n",
      "[11:58:03] - CPU: 4 cores\n",
      "[11:58:03] - memory: 16 GB\n",
      "\n",
      "[11:58:03] \u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
      "\n",
      "[11:58:03] Layer \u001b[1m1\u001b[0m train process start. Time left 2999.19 secs\n",
      "[11:58:04] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m ... Time budget is 1574.27 secs\n",
      "[12:01:57] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m completed\n",
      "[12:01:57] The set of hyperparameters \u001b[1m{'num_workers': 0, 'pin_memory': False, 'max_length': 256, 'is_snap': False, 'input_bn': False, 'max_emb_size': 256, 'bert_name': None, 'pooling': 'cls', 'device': ['0'], 'use_cont': True, 'use_cat': True, 'use_text': False, 'lang': 'en', 'deterministic': True, 'multigpu': False, 'random_state': 42, 'model': 'node', 'model_with_emb': False, 'path_to_save': None, 'verbose_inside': None, 'verbose': 1, 'n_epochs': 10, 'snap_params': {'k': 3, 'early_stopping': True, 'patience': 10, 'swa': True}, 'bs': 512, 'emb_dropout': 0.1, 'emb_ratio': 3, 'opt': 'Adam', 'opt_params': {'lr': 0.0003, 'weight_decay': 0}, 'sch': 'ReduceLROnPlateau', 'scheduler_params': {'patience': 5, 'factor': 0.5, 'min_lr': 1e-05}, 'loss': None, 'loss_params': {}, 'loss_on_logits': True, 'clip_grad': False, 'clip_grad_params': {}, 'init_bias': True, 'dataset': 'UniversalDataset', 'tuned': False, 'optimization_search_space': <function my_opt_space_NODE at 0x7fd4d11d1820>, 'verbose_bar': False, 'freeze_defaults': True, 'n_out': None, 'hid_factor': [2, 2], 'hidden_size': [512, 512, 512], 'block_config': [2, 2], 'compression': 0.5, 'growth_size': 256, 'bn_factor': 2, 'drop_rate': 0.12034524690886754, 'noise_std': 0.05, 'num_init_features': None, 'act_fun': 'ReLU', 'use_noise': False, 'use_bn': True, 'stop_by_metric': False, 'tuning_params': {'fit_on_holdout': True, 'max_tuning_iter': 25, 'max_tuning_time': 3600}, 'layer_dim': 512, 'use_original_head': False, 'num_layers': 3, 'tree_dim': 2}\u001b[0m\n",
      " achieve 0.7432 auc\n",
      "[12:01:57] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m ...\n",
      "[12:01:57] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m =====\n",
      "[12:02:09] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m =====\n",
      "[12:02:22] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m =====\n",
      "[12:02:34] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m =====\n",
      "[12:02:47] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m =====\n",
      "[12:02:59] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m finished. score = \u001b[1m0.7146780211829931\u001b[0m\n",
      "[12:02:59] \u001b[1mLvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0\u001b[0m fitting and predicting completed\n",
      "[12:02:59] Time left 2703.40 secs\n",
      "\n",
      "[12:02:59] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[12:02:59] \u001b[1mAutoml preset training completed in 296.61 seconds\u001b[0m\n",
      "\n",
      "[12:02:59] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (5 averaged models Lvl_0_Pipe_0_Mod_0_Tuned_TorchNN_node_tuned_0) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "oof_pred = automl.fit_predict(tr_data, roles = roles, verbose = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f82ac87",
   "metadata": {},
   "source": [
    "### 4.3 Several models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f70cafd6",
   "metadata": {},
   "source": [
    "If you have several neural networks you can either define one set parameters for all or use unique for each one of them as below.  \n",
    "**Note:** numeration starts with 0. Each id (string of number) corresponds to the serial number in *the list of used neural networks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d282b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:43:12] Stdout logging level is INFO3.\n",
      "[11:43:12] Task: binary\n",
      "\n",
      "[11:43:12] Start automl preset with listed constraints:\n",
      "[11:43:12] - time: 300.00 seconds\n",
      "[11:43:12] - CPU: 4 cores\n",
      "[11:43:12] - memory: 16 GB\n",
      "\n",
      "[11:43:12] \u001b[1mTrain data shape: (8000, 122)\u001b[0m\n",
      "\n",
      "[11:43:13] Feats was rejected during automatic roles guess: []\n",
      "[11:43:13] Layer \u001b[1m1\u001b[0m train process start. Time left 299.17 secs\n",
      "[11:43:13] Training until validation scores don't improve for 200 rounds\n",
      "[11:43:15] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[11:43:15] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m ...\n",
      "[11:43:15] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[11:43:15] Training until validation scores don't improve for 200 rounds\n",
      "[11:43:17] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[11:43:17] Training until validation scores don't improve for 200 rounds\n",
      "[11:43:24] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[11:43:24] Training until validation scores don't improve for 200 rounds\n",
      "[11:43:25] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[11:43:25] Training until validation scores don't improve for 200 rounds\n",
      "[11:43:27] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m =====\n",
      "[11:43:28] Training until validation scores don't improve for 200 rounds\n",
      "[11:43:30] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.7139016076564749\u001b[0m\n",
      "[11:43:30] \u001b[1mLvl_0_Pipe_0_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[11:43:30] Time left 281.41 secs\n",
      "\n",
      "[11:43:31] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_TorchNN_mlp_0\u001b[0m ...\n",
      "[11:43:31] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[11:43:31] Epoch: 0, train loss: 0.2787605822086334, val loss: 0.309201180934906, val metric: 0.6278257987608983\n",
      "[11:43:31] Epoch: 1, train loss: 0.27014315128326416, val loss: 0.3014439344406128, val metric: 0.6727606096081167\n",
      "[11:43:31] Early stopping: val loss: 0.3067486882209778, val metric: 0.6518273810478374\n",
      "[11:43:31] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[11:43:31] Epoch: 0, train loss: 0.2777830958366394, val loss: 0.25957152247428894, val metric: 0.6724667756453804\n",
      "[11:43:32] Epoch: 1, train loss: 0.26959046721458435, val loss: 0.2489766776561737, val metric: 0.7130498471467391\n",
      "[11:43:32] Early stopping: val loss: 0.2563818097114563, val metric: 0.6977326766304347\n",
      "[11:43:32] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[11:43:32] Epoch: 0, train loss: 0.27729859948158264, val loss: 0.26021498441696167, val metric: 0.5819967518682065\n",
      "[11:43:32] Epoch: 1, train loss: 0.2688000798225403, val loss: 0.2569577395915985, val metric: 0.6118960173233696\n",
      "[11:43:32] Early stopping: val loss: 0.25878584384918213, val metric: 0.5981498386548911\n",
      "[11:43:32] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[11:43:32] Epoch: 0, train loss: 0.2782357931137085, val loss: 0.2936389744281769, val metric: 0.6591664189877717\n",
      "[11:43:33] Epoch: 1, train loss: 0.2691851854324341, val loss: 0.2860284745693207, val metric: 0.6782173488451086\n",
      "[11:43:33] Early stopping: val loss: 0.29129818081855774, val metric: 0.6712646484375\n",
      "[11:43:33] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_0_TorchNN_mlp_0\u001b[0m =====\n",
      "[11:43:33] Epoch: 0, train loss: 0.278407484292984, val loss: 0.27651435136795044, val metric: 0.6396059782608697\n",
      "[11:43:33] Epoch: 1, train loss: 0.2713249921798706, val loss: 0.26803451776504517, val metric: 0.6795495074728259\n",
      "[11:43:33] Early stopping: val loss: 0.2736702263355255, val metric: 0.6605489979619565\n",
      "[11:43:33] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_TorchNN_mlp_0\u001b[0m finished. score = \u001b[1m0.649310571575994\u001b[0m\n",
      "[11:43:33] \u001b[1mLvl_0_Pipe_1_Mod_0_TorchNN_mlp_0\u001b[0m fitting and predicting completed\n",
      "[11:43:33] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_TorchNN_dense_1\u001b[0m ...\n",
      "[11:43:33] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_TorchNN_dense_1\u001b[0m =====\n",
      "[11:43:34] Epoch: 0, train loss: 0.2770945131778717, val loss: 0.30580174922943115, val metric: 0.6998920196075288\n",
      "[11:43:34] Epoch: 1, train loss: 0.24924148619174957, val loss: 0.286395400762558, val metric: 0.7347477695634278\n",
      "[11:43:34] Epoch: 2, train loss: 0.2179156094789505, val loss: 0.30268681049346924, val metric: 0.6945170550218902\n",
      "[11:43:35] Epoch: 3, train loss: 0.18389688432216644, val loss: 0.344427227973938, val metric: 0.6729530499115309\n",
      "[11:43:35] Epoch: 4, train loss: 0.15367864072322845, val loss: 0.385821133852005, val metric: 0.6347750319397448\n",
      "[11:43:35] Early stopping: val loss: 0.29431894421577454, val metric: 0.7355977142368406\n",
      "[11:43:35] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_TorchNN_dense_1\u001b[0m =====\n",
      "[11:43:35] Epoch: 0, train loss: 0.2714083790779114, val loss: 0.25767791271209717, val metric: 0.7117336107336957\n",
      "[11:43:36] Epoch: 1, train loss: 0.24716131389141083, val loss: 0.24509836733341217, val metric: 0.7210587211277173\n",
      "[11:43:36] Epoch: 2, train loss: 0.21747852861881256, val loss: 0.25955381989479065, val metric: 0.6777821416440216\n",
      "[11:43:36] Epoch: 3, train loss: 0.18614345788955688, val loss: 0.2698502540588379, val metric: 0.6448019276494565\n",
      "[11:43:37] Epoch: 4, train loss: 0.15456536412239075, val loss: 0.28265029191970825, val metric: 0.6477210003396741\n",
      "[11:43:37] Early stopping: val loss: 0.2492099106311798, val metric: 0.7247473675271738\n",
      "[11:43:37] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_TorchNN_dense_1\u001b[0m =====\n",
      "[11:43:37] Epoch: 0, train loss: 0.2723230719566345, val loss: 0.2602400481700897, val metric: 0.614937160326087\n",
      "[11:43:37] Epoch: 1, train loss: 0.2455139309167862, val loss: 0.2510344684123993, val metric: 0.6732390030570652\n",
      "[11:43:38] Epoch: 2, train loss: 0.21499498188495636, val loss: 0.26110419631004333, val metric: 0.6599227241847827\n",
      "[11:43:38] Epoch: 3, train loss: 0.18287943303585052, val loss: 0.2879733741283417, val metric: 0.6468452785326086\n",
      "[11:43:38] Epoch: 4, train loss: 0.15581558644771576, val loss: 0.30908963084220886, val metric: 0.6247930112092391\n",
      "[11:43:38] Early stopping: val loss: 0.2555948495864868, val metric: 0.665681258491848\n",
      "[11:43:38] ===== Start working with \u001b[1mfold 3\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_TorchNN_dense_1\u001b[0m =====\n",
      "[11:43:39] Epoch: 0, train loss: 0.274604856967926, val loss: 0.28871941566467285, val metric: 0.6861306895380436\n",
      "[11:43:39] Epoch: 1, train loss: 0.242709219455719, val loss: 0.271901398897171, val metric: 0.7212710173233696\n",
      "[11:43:39] Epoch: 2, train loss: 0.2166079580783844, val loss: 0.28201019763946533, val metric: 0.7093505859375001\n",
      "[11:43:40] Epoch: 3, train loss: 0.18465735018253326, val loss: 0.2982863187789917, val metric: 0.7004288383152174\n",
      "[11:43:40] Epoch: 4, train loss: 0.15224191546440125, val loss: 0.3260188102722168, val metric: 0.663733440896739\n",
      "[11:43:40] Early stopping: val loss: 0.27753010392189026, val metric: 0.7169985563858694\n",
      "[11:43:40] ===== Start working with \u001b[1mfold 4\u001b[0m for \u001b[1mLvl_0_Pipe_1_Mod_1_TorchNN_dense_1\u001b[0m =====\n",
      "[11:43:40] Epoch: 0, train loss: 0.2742379903793335, val loss: 0.274169921875, val metric: 0.7089684527853262\n",
      "[11:43:41] Epoch: 1, train loss: 0.2457791119813919, val loss: 0.2631347179412842, val metric: 0.7099344004755435\n",
      "[11:43:41] Epoch: 2, train loss: 0.2198062241077423, val loss: 0.2773990035057068, val metric: 0.6714291779891304\n",
      "[11:43:41] Epoch: 3, train loss: 0.1879502236843109, val loss: 0.32326072454452515, val metric: 0.6654795771059782\n",
      "[11:43:41] Epoch: 4, train loss: 0.1601661890745163, val loss: 0.357746422290802, val metric: 0.6396059782608696\n",
      "[11:43:42] Early stopping: val loss: 0.266778826713562, val metric: 0.7362511676290762\n",
      "[11:43:42] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_TorchNN_dense_1\u001b[0m finished. score = \u001b[1m0.7106457519741463\u001b[0m\n",
      "[11:43:42] \u001b[1mLvl_0_Pipe_1_Mod_1_TorchNN_dense_1\u001b[0m fitting and predicting completed\n",
      "[11:43:42] Time left 270.04 secs\n",
      "\n",
      "[11:43:42] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:43:42] Blending: optimization starts with equal weights and score \u001b[1m0.73113620210903\u001b[0m\n",
      "[11:43:42] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.7330658618498413\u001b[0m, weights = \u001b[1m[0.35163662 0.06402954 0.58433384]\u001b[0m\n",
      "[11:43:42] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.7332030948540493\u001b[0m, weights = \u001b[1m[0.371031 0.       0.628969]\u001b[0m\n",
      "[11:43:42] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.7332030948540493\u001b[0m, weights = \u001b[1m[0.371031 0.       0.628969]\u001b[0m\n",
      "[11:43:42] Blending: no score update. Terminated\n",
      "\n",
      "[11:43:42] \u001b[1mAutoml preset training completed in 30.17 seconds\u001b[0m\n",
      "\n",
      "[11:43:42] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.37103 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LightGBM) +\n",
      "\t 0.62897 * (5 averaged models Lvl_0_Pipe_1_Mod_1_TorchNN_dense_1) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.08149866],\n",
       "       [0.04592865],\n",
       "       [0.04751563],\n",
       "       ...,\n",
       "       [0.06561954],\n",
       "       [0.15983571],\n",
       "       [0.11311316]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = TabularAutoML(\n",
    "    **default_lama_params,\n",
    "    general_params = {\"use_algos\": [[\"lgb\", \"mlp\", \"dense\"]]},\n",
    "    nn_params = {\"0\": {**default_nn_params, \"n_epochs\": 2},\n",
    "                 \"1\": {**default_nn_params, \"n_epochs\": 5}},\n",
    ")\n",
    "automl.fit_predict(tr_data, roles = roles, verbose = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1531.539656,
   "end_time": "2021-06-22T20:35:52.076563",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-22T20:10:20.536907",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "77738023c946bcedeeb6a5c983bcfb7849325694ebd87acbc9267f45e9f4af48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
